{"capabilityDescription":{"en":"Consumes messages from Apache Kafka Consumer API. The complementary NiFi processor for sending messages is PublishKafka. The Processor supports consumption of Kafka messages, optionally interpreted as NiFi records. Please note that, at this time (in read record mode), the Processor assumes that all records that are retrieved from a given partition have the same schema. For this mode, if any of the Kafka messages are pulled but cannot be parsed or written with the configured Record Reader or Record Writer, the contents of the message will be written to a separate FlowFile, and that FlowFile will be transferred to the 'parse.failure' relationship. Otherwise, each FlowFile is sent to the 'success' relationship and may contain many individual messages within the single FlowFile. A 'record.count' attribute is added to indicate how many messages are contained in the FlowFile. No two Kafka messages will be placed into the same FlowFile if they have different schemas, or if they have different values for a message header that is included by the <Headers to Add as Attributes> property.","zh":"Consumes messages from Apache Kafka Consumer API. The complementary NiFi processor for sending messages is PublishKafka. The Processor supports consumption of Kafka messages, optionally interpreted as NiFi records. Please note that, at this time (in read record mode), the Processor assumes that all records that are retrieved from a given partition have the same schema. For this mode, if any of the Kafka messages are pulled but cannot be parsed or written with the configured Record Reader or Record Writer, the contents of the message will be written to a separate FlowFile, and that FlowFile will be transferred to the 'parse.failure' relationship. Otherwise, each FlowFile is sent to the 'success' relationship and may contain many individual messages within the single FlowFile. A 'record.count' attribute is added to indicate how many messages are contained in the FlowFile. No two Kafka messages will be placed into the same FlowFile if they have different schemas, or if they have different values for a message header that is included by the <Headers to Add as Attributes> property."},"properties":{"Topics":{"en":{"displayName":"Topics","description":"The name or pattern of the Kafka Topics from which the Processor consumes Kafka Records. More than one can be supplied if comma separated."},"zh":{"description":"The name or pattern of the Kafka Topics from which the Processor consumes Kafka Records. More than one can be supplied if comma separated.","displayName":"Topics"}},"Commit Offsets":{"en":{"displayName":"Commit Offsets","description":"Specifies whether this Processor should commit the offsets to Kafka after receiving messages. Typically, this value should be set to true so that messages that are received are not duplicated. However, in certain scenarios, we may want to avoid committing the offsets, that the data can be processed and later acknowledged by PublishKafka in order to provide Exactly Once semantics."},"zh":{"description":"Specifies whether this Processor should commit the offsets to Kafka after receiving messages. Typically, this value should be set to true so that messages that are received are not duplicated. However, in certain scenarios, we may want to avoid committing the offsets, that the data can be processed and later acknowledged by PublishKafka in order to provide Exactly Once semantics.","displayName":"提交偏移量"}},"Header Name Pattern":{"en":{"displayName":"Header Name Pattern","description":"Regular Expression Pattern applied to Kafka Record Header Names for selecting Header Values to be written as FlowFile attributes"},"zh":{"description":"Regular Expression Pattern applied to Kafka Record Header Names for selecting Header Values to be written as FlowFile attributes","displayName":"Header Name Pattern"}},"Key Format":{"en":{"displayName":"Key Format","description":"Specifies how to represent the Kafka Record Key in the output FlowFile"},"zh":{"description":"Specifies how to represent the Kafka Record Key in the output FlowFile","displayName":"密钥格式"}},"Key Record Reader":{"en":{"displayName":"Key Record Reader","description":"The Record Reader to use for parsing the Kafka Record Key into a Record"},"zh":{"description":"The Record Reader to use for parsing the Kafka Record Key into a Record","displayName":"密钥记录读取器"}},"Message Demarcator":{"en":{"displayName":"Message Demarcator","description":"Since KafkaConsumer receives messages in batches, this Processor has an option to output FlowFiles which contains all Kafka messages in a single batch for a given topic and partition and this property allows you to provide a string (interpreted as UTF-8) to use for demarcating apart multiple Kafka messages. This is an optional property and if not provided each Kafka message received will result in a single FlowFile which  time it is triggered. To enter special character such as 'new line' use CTRL+Enter or Shift+Enter depending on the OS"},"zh":{"description":"Since KafkaConsumer receives messages in batches, this Processor has an option to output FlowFiles which contains all Kafka messages in a single batch for a given topic and partition and this property allows you to provide a string (interpreted as UTF-8) to use for demarcating apart multiple Kafka messages. This is an optional property and if not provided each Kafka message received will result in a single FlowFile which  time it is triggered. To enter special character such as 'new line' use CTRL+Enter or Shift+Enter depending on the OS","displayName":"消息标定器"}},"Record Reader":{"en":{"displayName":"Record Reader","description":"The Record Reader to use for incoming Kafka messages"},"zh":{"description":"The Record Reader to use for incoming Kafka messages","displayName":"记录读取器"}},"Key Attribute Encoding":{"en":{"displayName":"Key Attribute Encoding","description":"Encoding for value of configured FlowFile attribute containing Kafka Record Key."},"zh":{"description":"Encoding for value of configured FlowFile attribute containing Kafka Record Key.","displayName":"关键属性编码"}},"Topic Format":{"en":{"displayName":"Topic Format","description":"Specifies whether the Topics provided are a comma separated list of names or a single regular expression"},"zh":{"description":"Specifies whether the Topics provided are a comma separated list of names or a single regular expression","displayName":"Topic Format"}},"Header Encoding":{"en":{"displayName":"Header Encoding","description":"Character encoding applied when reading Kafka Record Header values and writing FlowFile attributes"},"zh":{"description":"Character encoding applied when reading Kafka Record Header values and writing FlowFile attributes","displayName":"Header Encoding"}},"Max Uncommitted Time":{"en":{"displayName":"Max Uncommitted Time","description":"Specifies the maximum amount of time allowed to pass before offsets must be committed. This value impacts how often offsets will be committed. Committing offsets less often increases throughput but also increases the window of potential data duplication in the event of a rebalance or JVM restart between commits. This value is also related to maximum poll records and the use of a message demarcator. When using a message demarcator we can have far more uncommitted messages than when we're not as there is much less for us to keep track of in memory."},"zh":{"description":"Specifies the maximum amount of time allowed to pass before offsets must be committed. This value impacts how often offsets will be committed. Committing offsets less often increases throughput but also increases the window of potential data duplication in the event of a rebalance or JVM restart between commits. This value is also related to maximum poll records and the use of a message demarcator. When using a message demarcator we can have far more uncommitted messages than when we're not as there is much less for us to keep track of in memory.","displayName":"最长未提交时间"}},"Kafka Connection Service":{"en":{"displayName":"Kafka Connection Service","description":"Provides connections to Kafka Broker for publishing Kafka Records"},"zh":{"description":"Provides connections to Kafka Broker for publishing Kafka Records","displayName":"Kafka Connection Service"}},"Separate By Key":{"en":{"displayName":"Separate By Key","description":"When this property is enabled, two messages will only be added to the same FlowFile if both of the Kafka Messages have identical keys."},"zh":{"description":"When this property is enabled, two messages will only be added to the same FlowFile if both of the Kafka Messages have identical keys.","displayName":"按关键字分隔"}},"Processing Strategy":{"en":{"displayName":"Processing Strategy","description":"Strategy for processing Kafka Records and writing serialized output to FlowFiles"},"zh":{"description":"Strategy for processing Kafka Records and writing serialized output to FlowFiles","displayName":"Processing Strategy"}},"Record Writer":{"en":{"displayName":"Record Writer","description":"The Record Writer to use in order to serialize the outgoing FlowFiles"},"zh":{"description":"The Record Writer to use in order to serialize the outgoing FlowFiles","displayName":"记录编写器"}},"Group ID":{"en":{"displayName":"Group ID","description":"Kafka Consumer Group Identifier corresponding to Kafka group.id property"},"zh":{"description":"Kafka Consumer Group Identifier corresponding to Kafka group.id property","displayName":"组ID"}},"auto.offset.reset":{"en":{"displayName":"Auto Offset Reset","description":"Automatic offset configuration applied when no previous consumer offset found corresponding to Kafka auto.offset.reset property"},"zh":{"description":"Automatic offset configuration applied when no previous consumer offset found corresponding to Kafka auto.offset.reset property","displayName":"Auto Offset Reset"}},"Output Strategy":{"en":{"displayName":"Output Strategy","description":"The format used to output the Kafka Record into a FlowFile Record."},"zh":{"description":"The format used to output the Kafka Record into a FlowFile Record.","displayName":"输出策略"}}},"relationships":{"success":{"en":"FlowFiles containing one or more serialized Kafka Records","zh":"FlowFiles containing one or more serialized Kafka Records"}},"tags":{"en":["Kafka","Get","Record","csv","avro","json","Ingest","Ingress","Topic","PubSub","Consume"],"zh":["卡夫卡","收到","记录","csv格式","我没有吗？","json文件","摄入","进入","话题","发布Sub","消费"]},"writeAttributes":{"kafka.tombstone":{"en":"Set to true if the consumed message is a tombstone message","zh":"Set to true if the consumed message is a tombstone message"},"kafka.partition":{"en":"The partition of the topic the message or message bundle is from","zh":"消息或消息包所在主题的分区"},"record.count":{"en":"The number of records received","zh":"收到的记录数"},"kafka.offset":{"en":"The offset of the message in the partition of the topic.","zh":"主题分区中消息的偏移量。"},"kafka.count":{"en":"The number of messages written if more than one","zh":"如果多于一条，则写入的消息数"},"kafka.timestamp":{"en":"The timestamp of the message in the partition of the topic.","zh":"主题分区中消息的时间戳。"},"kafka.topic":{"en":"The topic the message or message bundle is from","zh":"消息或消息包的主题"},"mime.type":{"en":"The MIME Type that is provided by the configured Record Writer","zh":"配置的记录编写器提供的MIME类型"},"kafka.key":{"en":"The key of message if present and if single message. How the key is encoded depends on the value of the 'Key Attribute Encoding' property.","zh":"消息密钥（如果存在）和单个消息。密钥的编码方式取决于“key Attribute Encoding”属性的值。"}}}